{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T1M3qlQlhSu"
      },
      "source": [
        "# Stanford CS 329a Self-Improving AI Agents, Homework 3\n",
        "\n",
        "This assignment builds an end-to-end agentic workflow by combining LLMs with external APIs and self-improvement techniques.\n",
        "\n",
        "- **Part 1:** Implementing an API-augmented LLM pipeline which integrates four external APIs, and generates responses grounded in API responses.\n",
        "- **Part 2:** Applying self-improvement methods:\n",
        "  - Query decomposition & fusion: Breaks complex queries into sub-queries, retrieves API outputs, and fuses the results.\n",
        "  - Iterative refinement: Uses the LLM to trigger additional API calls and refine the response until sufficient information is gathered to answer the query.\n",
        "- **Part 3:** Integrating these components into a full agentic workflow and evaluating accuracy on the test set.\n",
        "- **Part 4:** Extending the system into a deep research agent for knowledge-intensive tasks, inspired by recent AI launches.\n",
        "\n",
        "**Final Deliverable**: A zipped folder (.zip) of your edited files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Q5ptwolhSv"
      },
      "outputs": [],
      "source": [
        "# Auto-reloads imported modules, so changes to .py files are automatically reflected in the notebook\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs9oF7xNlhSv"
      },
      "source": [
        "## Package Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu1ST_gdlhSw"
      },
      "source": [
        "#### API Configuration and Setup\n",
        "\n",
        "Before we can use the APIs, we need to set up our environment and initialize the API manager. This involves:\n",
        "\n",
        "1. Loading API keys from environment variables\n",
        "2. Setting up the API manager with the necessary credentials\n",
        "3. Validating that all required keys are present\n",
        "\n",
        "For this homework, we'll use:\n",
        "- Google Custom Search API (requires API key and Custom Search Engine ID):\n",
        "     - https://developers.google.com/custom-search/v1/overview\n",
        "     - https://programmablesearchengine.google.com/controlpanel/create\n",
        "       -  Once created, the Custom Search Engine ID can be found here: https://programmablesearchengine.google.com/controlpanel/all\n",
        "- Polygon API (requires API key):\n",
        "     - https://polygon.io/dashboard/keys\n",
        "- Wolfram Alpha API (requires app ID): \n",
        "     - https://developer.wolframalpha.com/access\n",
        "\n",
        "**Note**: Be mindful of your API usage and compute budget. Use smaller, more cost-effective models (e.g. gemma-3n-E4B-it) for development before scaling to larger models. For security, never hardcode API keys; always use a `.env` file and add it to your `.gitignore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwDagCRflhSw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from cs329a_hw3.api_manager import APIManager\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get API keys\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "GOOGLE_CX_ID = os.getenv('GOOGLE_CX_ID')\n",
        "POLYGON_API_KEY = os.getenv('POLYGON_API_KEY')\n",
        "TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n",
        "WOLFRAM_APP_ID = os.getenv('WOLFRAM_APP_ID')\n",
        "\n",
        "if not all([GOOGLE_API_KEY, GOOGLE_CX_ID, POLYGON_API_KEY, TOGETHER_API_KEY, WOLFRAM_APP_ID]):\n",
        "    raise ValueError(\"One or more required API keys are missing from environment variables\")\n",
        "\n",
        "api_manager = APIManager(\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    google_cx_id=GOOGLE_CX_ID,\n",
        "    polygon_api_key=POLYGON_API_KEY,\n",
        "    wolfram_app_id=WOLFRAM_APP_ID\n",
        ")\n",
        "\n",
        "print(\"APIManager initialized successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow3h0bBtlhSx"
      },
      "source": [
        "## Part 0 - LLM Performance with a Single Call [5 points]\n",
        "\n",
        "To establish a baseline, let's test the performance of a standalone Large Language Model (LLM) without any external tools or APIs. This helps illustrate the limitations of relying solely on the model's internal knowledge, especially for questions requiring real-time, specific, or computational information.\n",
        "\n",
        "**Deliverable:**\n",
        "- In the `cs329a_hw3/multi_lm_agent.py` file, implement the `generate` method in the MultiLMAgent class.\n",
        "\n",
        "**Observe that:**\n",
        "- LLMs can hallucinate or provide outdated information for fact-based queries.\n",
        "- Models often struggle with precise mathematical calculations that a tool like Wolfram Alpha can solve instantly, due to the tokenization scheme.\n",
        "- Queries about current stock prices or today's weather are outside the scope of a model's static training data.\n",
        "- Evaluation is difficult for unstructured model outputs. Numerical answers have multiple possible formats, which makes naive string-matching unreliable; and natural language answers can be phrased in various ways. To that end, we've provided a `evaluate_qa` function that uses a fast, cheap model (gemma-3n-E4B-it) to more reliably evaluate the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v45Wk6H5lhSx"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.multi_lm_agent import MultiLMAgent\n",
        "from cs329a_hw3.evaluation import prepare_dataset, evaluate_qa\n",
        "\n",
        "multi_lm_agent = MultiLMAgent(api_manager)\n",
        "dataset = prepare_dataset(debug_mode=False)\n",
        "\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "num_correct = 0\n",
        "print(\"Zero-Shot Responses:\")\n",
        "zero_shot_responses = []\n",
        "for query, answer in zip(queries, answers):\n",
        "    response = multi_lm_agent.generate(query=query, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")\n",
        "    zero_shot_responses.append(response)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Zero-Shot Response: {response}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    is_correct = evaluate_qa(query, response, answer, model=\"google/gemma-3n-E4B-it\")\n",
        "    num_correct += is_correct\n",
        "    print(\"Is Correct:\", is_correct)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "accuracy_zero_shot = num_correct / len(queries)\n",
        "print(f\"Accuracy = {accuracy_zero_shot*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE5jRypG1aEm"
      },
      "source": [
        "## Part 1 - API-augmented LLM pipeline [40 points]\n",
        "\n",
        "In this part, we augment the LLM with four different API calls, then implement an LLM-based API router that routes the query to the appropriate API, and then prompt the LLM to generate the final response given the API outputs.\n",
        "\n",
        "We will work with the following APIs and select the appropriate API for a given query:\n",
        "\n",
        "1. **Google Custom Search API** - For web search\n",
        "2. **Polygon API** - For financial data\n",
        "3. **Wolfram Alpha API** - For mathematical calculations\n",
        "4. **Weather API** - For location-based weather data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBW_rgJlhSw"
      },
      "source": [
        "#### 1a. Google Custom Search API [5 points]\n",
        "\n",
        "The Google Custom Search API allows us to programmatically search the web. We'll use this to gather information and context for our tasks.\n",
        "\n",
        "Key features:\n",
        "- Web search with customizable parameters\n",
        "- Filtering and sorting options\n",
        "- Rich metadata about search results\n",
        "\n",
        "Deliverable: In the ``cs329_hw3/api_manager.py`` file, implement the `_extract_webpage_content` and ``google_search`` functions.\n",
        "\n",
        "**Note**: The `google_search` function will return long webpages, so we will need to truncate or parse the response to get the relevant information. Otherwise, the added context will exceed the context window of the LMs in later functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KcoHwuClhSw"
      },
      "outputs": [],
      "source": [
        "search_query = \"Apple Product News\"\n",
        "results = api_manager.google_search(\n",
        "    search_query=search_query,\n",
        "    num_results=3\n",
        ")\n",
        "\n",
        "print(\"\\nSearch results for:\", search_query)\n",
        "print(\"-\" * 50)\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    for k, v in result.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZH5yIpMlhSw"
      },
      "source": [
        "#### 1b. Polygon API [5 points]\n",
        "\n",
        "The Polygon API provides real-time and historical financial data. We'll use this for analyzing stock market information.\n",
        "\n",
        "Key features:\n",
        "- Real-time stock quotes\n",
        "- Historical price data\n",
        "- Technical indicators\n",
        "- Company fundamentals\n",
        "\n",
        "Deliverable: In the ``cs329a_hw3/api_manager.py`` file, implement the ``get_stock_data`` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cjmJmwmlhSw"
      },
      "outputs": [],
      "source": [
        "ticker, date = \"TSLA\", \"2025-08-27\"\n",
        "stock_data = api_manager.get_stock_data(ticker=ticker, date=date)\n",
        "if isinstance(stock_data, dict):\n",
        "    print(f\"\\nStock data for {ticker} on {date}:\")\n",
        "    for k, v in stock_data.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "else:\n",
        "    print(\"\\tNo stock data available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "196BcY7alhSw"
      },
      "source": [
        "#### 1c. Wolfram Alpha API [5 points]\n",
        "\n",
        "The Wolfram Alpha API provides powerful computational capabilities to answer mathematical queries.\n",
        "\n",
        "**Deliverable**: In the ``cs329a_hw3/api_manager.py`` file, implement the `compute` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9COV1K0xlhSx"
      },
      "outputs": [],
      "source": [
        "math_query = \"integral cos(x)/sqrt(x) from 0 to 1\"\n",
        "result = api_manager.compute(math_query)\n",
        "print(f\"Wolfram Alpha result for {math_query}: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8ywSe9LlhSx"
      },
      "source": [
        "#### 1d. Weather API [5 points]\n",
        "\n",
        "The Weather API provides historical weather data for any location, including temperature, precipitation, wind, etc.\n",
        "\n",
        "To impement this, we'll use Nominatim from the [Geopy API](https://geopy.readthedocs.io/en/stable/) to geocode a location, and then use the [Open-Meteo API](https://open-meteo.com/) to get the weather data for that location.\n",
        "\n",
        "Deliverable: In the ``cs329a_hw3/api_manager.py`` file, implement the ``get_weather`` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM0nrG0SlhSx"
      },
      "outputs": [],
      "source": [
        "# We can use the API manager's weather functionality\n",
        "from datetime import datetime\n",
        "location = \"Palo Alto, CA\"\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "hour = \"14\"\n",
        "\n",
        "weather_data = api_manager.get_weather(location, date, hour)\n",
        "if \"error\" not in weather_data:\n",
        "    print(f\"Weather conditions for {location} on {date} at {hour}:00:\")\n",
        "    for k, v in weather_data.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "else:\n",
        "    print(f\"\\tCould not get weather data for {location}: {weather_data['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj37BiAIlhSx"
      },
      "source": [
        "#### 1e. API Routing [15 points]\n",
        "\n",
        "The API routing is a system that uses language models to route queries to the appropriate API function. This allows us to build an agent that can use multiple APIs to answer user queries.\n",
        "\n",
        "Key requirements for the function logic and prompt construction:\n",
        "- Query an LLM to determine the appropriate API to use for the query\n",
        "- Correctly parse the query response and map it to the appropriate API function\n",
        "- In the query response, include the API name and parameters to be used\n",
        "- Query the selected API and return the response from the API after parsing\n",
        "- Handle edge cases and fallbacks for query parsing and API selection\n",
        "\n",
        "Deliverable: In the ``cs329a_hw3/api_manager.py`` file, implement the ``_parse_query_params`` and ``route_query`` functions.\n",
        "\n",
        "Note: TogetherAI supports structured outputs, which can simplify the implementation of these functions: https://docs.together.ai/docs/json-mode.\n",
        "We use Pydantic models to define the expected parameters for each API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LxUoUTdlhSx"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.evaluation import prepare_dataset\n",
        "\n",
        "dataset = prepare_dataset(debug_mode=True)\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    output = api_manager.route_query(query, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")\n",
        "    print(f\"Called `{output['api_used']}` with parameters: {output['params']}\")\n",
        "    print(\"Result:\", output[\"results\"])\n",
        "    print(\"API Used:\", output[\"api_used\"])\n",
        "    print(\"-\" * 50)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8SAOb3c39fg"
      },
      "source": [
        "#### 1f. Evaluation with a single API call [5 points]\n",
        "\n",
        "With the API router complete, let's evaluate its performance. This involves routing a query to the correct API, getting the data, and then prompting an LLM with both the original query and the API data to generate a final answer.\n",
        "\n",
        "**Deliverable:**\n",
        "- In `cs329a_hw3/multi_lm_agent.py`, implement the `single_LM_with_single_API_call` function.`\n",
        "\n",
        "Key requirements for the function prompt and logic:\n",
        "- Query the API manager for the necessary data\n",
        "- Use the query and the data retrieved from the API manager to create a prompt for the model\n",
        "- Use the model to generate the response\n",
        "- Return the response from the model\n",
        "\n",
        "Deliverable: In the `cs329a_hw3/multi_lm_agent.py` file, implement the `single_LM_with_single_API_call` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDW-NL39lhSy"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.evaluation import evaluate_qa, prepare_dataset\n",
        "\n",
        "multi_lm_agent = MultiLMAgent(api_manager)\n",
        "dataset = prepare_dataset(debug_mode=True)\n",
        "\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "num_correct = 0\n",
        "\n",
        "print(\"Single-Call LM with Single API Call:\")\n",
        "single_LM_with_single_API_call_responses = []\n",
        "for query, answer in zip(queries, answers):\n",
        "    response = multi_lm_agent.single_LM_with_single_API_call(query=query, model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\")\n",
        "    single_LM_with_single_API_call_responses.append(response)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Single-Call LM Response: {response}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    is_correct = evaluate_qa(query, response, answer, model=\"google/gemma-3n-E4B-it\")\n",
        "    num_correct += is_correct\n",
        "    print(\"Is Correct:\", is_correct)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "accuracy_singleLM_with_single_API_call = num_correct / len(queries)\n",
        "print(f\"Accuracy = {accuracy_singleLM_with_single_API_call*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ9lEYYe4ilL"
      },
      "source": [
        "## Part 2 - Self-improvement techniques [40 points]\n",
        "We now improve the accuracy by using a) **query decomposition and fusion** and b) **iterative self-refinement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ctH4gJlhSx"
      },
      "source": [
        "#### 2a. Query Decomposition [10 points]\n",
        "\n",
        "Complex queries often require information from multiple API calls (e.g., \"Which city was windier, Chicago or Boston?\"). To address such queries, we will first create the Query Decomposition component, which breaks down complex queries into simpler, more manageable parts, allowing us to use multiple APIs to answer the query.\n",
        "\n",
        "Key requirements:\n",
        "- Use the LLM to decompose the query into multiple independent sub-queries relevant for answering the original query (`_get_query_decomposition_prompt` and `_get_sub_queries` in `multi_lm_agent.py`)\n",
        "- Route each sub-query to the appropriate API using the API manager, and gather the structured API results and parameters from each sub-query\n",
        "- Error handling for failed decompositions, failed API calls, and failed query parsing\n",
        "\n",
        "Deliverable: In the `cs329a_hw3/multi_lm_agent.py` file, implement the `decompose_query`; as discussed above, this also requires implementing the `_get_query_decomposition_prompt` and `_get_sub_queries` helper functions.\n",
        "\n",
        "**Note**:\n",
        "- Using a `ThreadPoolExecutor` in your `decompose_query` implementation can significantly speed up execution by making multiple API calls concurrently.\n",
        "- Google Search webpage results can be very long, so they must be truncated or parsed to extract the relevant information and avoid exceeding the context window of the LMs.\n",
        "- The `decompose_query` method takes a query and returns a list of sub-queries. How do these sub-queries help with the overall task? What information do they provide that the original query does not? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRXAturVlhSx"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.multi_lm_agent import MultiLMAgent\n",
        "from cs329a_hw3.evaluation import prepare_dataset, evaluate_qa\n",
        "\n",
        "multi_lm_agent = MultiLMAgent(api_manager)\n",
        "\n",
        "dataset = prepare_dataset(debug_mode=True)\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "\n",
        "print(\"Testing Query Decomposition:\")\n",
        "decomposed_queries = []  # List[List[Dict]], where each inner list contains the decomposed queries for a single query\n",
        "for query in queries:\n",
        "    print(f\"Original Query: {query}\")\n",
        "    decomposed_query = multi_lm_agent.decompose_query(query=query, max_sub_queries=3)\n",
        "\n",
        "    print(\"Sub-queries:\")\n",
        "    for sub_query in decomposed_query:\n",
        "        for k, v in sub_query.items():\n",
        "            print(f\"\\t{k}: {v}\")\n",
        "        print()\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    decomposed_queries.append(decomposed_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY4wfZGYlhSy"
      },
      "source": [
        "#### 2b. Synthesizing information from sub-query API responses [5 points]\n",
        "\n",
        "After executing the sub-queries, we need to assemble a new, context-rich prompt that incorporates each sub-query's API response and the user's query. This will be given to a model to synthesize a final answer.\n",
        "\n",
        "Deliverable: In the `cs329a_hw3/multi_lm_agent.py` file, implement `_get_synthesis_prompt`.\n",
        "\n",
        "**Note**:\n",
        "- Do the prompts constructed provide all the necessary information to answer the query?\n",
        "- A good synthesis prompt should clearly present the original query and  neatly format the results from each successful API call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKJZWHRVlhSy"
      },
      "outputs": [],
      "source": [
        "dataset = prepare_dataset(debug_mode=True)\n",
        "\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "generated_prompts = []\n",
        "for query, decomposed_query in zip(queries, decomposed_queries):\n",
        "    generated_prompt = multi_lm_agent._get_synthesis_prompt(query, decomposed_query)\n",
        "    print(\"Generated Prompt:\")\n",
        "    print(generated_prompt)\n",
        "    print(\"-\" * 50)\n",
        "    generated_prompts.append(generated_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRYpZTtMlhSy"
      },
      "source": [
        "#### 2c. Fusion of responses [5 points]\n",
        "\n",
        "With the constructed prompt, we can generate multiple responses with different models to get a diverse set of responses, and then use a fusion model to combine the best elements from each into a single coherent output. This generally provides a more robust, comprehensive response than any single model could provide.\n",
        "\n",
        "Key requirements for prompt construction:\n",
        "- Call `decompose_query` and `_get_synthesis_prompt` to get the decomposed queries and the generated synthesis prompt (which contains the original query and the results from each sub-query)\n",
        "- Query multiple models with the generated synthesis prompt (specifically `\"google/gemma-3n-E4B-it\"`, `\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"`, and `\"OpenAI/gpt-oss-20B\"`)\n",
        "- Combine elements from these multiple responses by creating a new fusion prompt, while maintaining consistency and clarity in the final response\n",
        "- Handle edge cases and fallbacks\n",
        "- Return the final response from the fusion model\n",
        "\n",
        "Deliverable: In the `cs329_hw3/multi_lm_agent.py` file, implement the `decompose_and_fuse`.This function orchestrates the full query decomposition and fusion workflow. It should make internal calls to `decompose_query`, `_get_synthesis_prompt`, and `generate`. For the fusion step, you should sample from in parallel, then feed their responses to the final fusion model.\n",
        "\n",
        "**Note**: The `fuse` method should take the generated prompt and multiple generated responses before returning a single fused response. Compare how the fused response is different from the individual responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixZRAS2VlhSy"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.evaluation import evaluate_qa, prepare_dataset\n",
        "\n",
        "dataset = prepare_dataset(debug_mode=True)\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "\n",
        "print(\"Generated Responses with Query Decomposition + Fusion:\")\n",
        "query_decomp_responses = []\n",
        "num_correct = 0\n",
        "for query, answer in zip(queries, answers):\n",
        "    response = multi_lm_agent.decompose_and_fuse(query)\n",
        "    query_decomp_responses.append(response)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Query Decomposition + Fusion Response: {response}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    is_correct = evaluate_qa(query, response, answer, model=\"google/gemma-3n-E4B-it\")\n",
        "    num_correct += is_correct\n",
        "    print(\"Is Correct:\", is_correct)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "accuracy_query_decomp = num_correct / len(queries)\n",
        "print(f\"Accuracy = {accuracy_query_decomp*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MeFCmOplhSy"
      },
      "source": [
        "#### 2d. Iterative Self-Refinement [20 points]\n",
        "\n",
        "Sometimes a single round of decomposition is not enough to answer the query. To address this, Iterative Refinement improves the response by querying for more information as needed across multiple APIs.\n",
        "\n",
        "At each step, the agent can either issue a new sub-query to gather more information (e.g. if a prior sub-query failed or if additional information is needed); or generate the final answer if it has enough information. This can be particularly useful for complex queries that require multiple API calls to answer, such as multi-hop question-answering where the result of one step informs the next.\n",
        "\n",
        "Deliverable: In `cs329a_hw3/multi_lm_agent.py`, implement the `iterative_refine` method.\n",
        "\n",
        "Key requirements for function logic and prompt construction:\n",
        "- At each step, you should construct a prompt with the user query, cleanly formatted API results of all previous sub-queries, and instructions for the LLM to either issue a new sub-query or generate the final answer (`_get_iterative_refinement_prompt` of `cs329a_hw3/multi_lm_agent.py`)\n",
        "- Based on the LLM's response, you should either get an API response for a new sub-query (using the `route_query` method) or return its response as the final answer.\n",
        "- Exit when the model outputs a final answer, or when the maximum number of iterations is reached.\n",
        "\n",
        "Note unlike query decomposition + fusion, the iterative refinement module allows for sub-queries to be re-issued if an earlier sub-query failed, as well as for intermediate information to be incorporated into subsequent sub-queries.\n",
        "\n",
        "For resources on LM judges and self-verification, see: \n",
        "- [LM Judge Survey](https://arxiv.org/abs/2411.15594)\n",
        "- [Large Language Models are Better Reasoners with Self-Verification](https://arxiv.org/abs/2212.09561)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zws1bpWplhSy"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.evaluation import evaluate_qa, prepare_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset = prepare_dataset(debug_mode=True)\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "\n",
        "multi_lm_agent = MultiLMAgent(api_manager)\n",
        "print(\"Generated Responses with Iterative Refinement:\")\n",
        "iterative_refine_responses = []\n",
        "num_correct = 0\n",
        "for query, answer in tqdm(zip(queries, answers)):\n",
        "    print(f\"Query: {query}\")\n",
        "    \n",
        "    response = multi_lm_agent.iterative_refine(query, max_iterations=4)\n",
        "    print(f\"Iterative Refinement Response: {response}\")\n",
        "    iterative_refine_responses.append(response)\n",
        "    \n",
        "    print(f\"Answer: {answer}\")\n",
        "    is_correct = evaluate_qa(query, response, answer, model=\"google/gemma-3n-E4B-it\")\n",
        "    num_correct += is_correct\n",
        "    print(\"Is Correct:\", is_correct)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "accuracy_iterative_refine = num_correct / len(queries)\n",
        "print(f\"Accuracy = {accuracy_iterative_refine*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a bar graph plotting the accuracies of zero-shot prompting, a single API router and LM call, query decomposition and fusion, and iterative refinement (max 4 rounds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJKaRSYGlhSy"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create data for the bar graph\n",
        "methods = ['Zero-shot', 'Single API router and LM call', 'Query decomposition and fusion', 'Iterative refinement']\n",
        "\n",
        "# Must be in this order and in floating point format from 0.0 to 1.0\n",
        "accuracies = [\n",
        "    accuracy_zero_shot / 100,\n",
        "    accuracy_singleLM_with_single_API_call / 100, \n",
        "    accuracy_query_decomp / 100,\n",
        "    accuracy_iterative_refine / 100\n",
        "]\n",
        "\n",
        "colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(methods, accuracies, color=colors)\n",
        "\n",
        "plt.title('Accuracy Comparison Across Different LLM Pipelines')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2%}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBNcopTVlhSy"
      },
      "source": [
        "## Part 3 - Building an LLM Agentic Workflow [15 points]\n",
        "\n",
        "Using the components we've built (or new ones you've implemented), build an LLM agent that can answer the following questions about the world. These questions will be a mix of the types of questions we've built components for and will require using the APIs in creative ways.\n",
        "\n",
        "Some questions will require just a single API call, while others will require multiple API calls and multiple rounds of iterative refinement. Create a pipeline that can dynamically adjust to the complexity of the question. Feel free to implement new components or use the ones we've already built!\n",
        "\n",
        "If you get above 70% accuracy on the entire dataset, you will get full points. For scores below 70%, you will get partial credit based on the percentage of accuracy.\n",
        "\n",
        "**Important**: Make sure to evaluate over the entire dataset when you are confident with your implementation! This will help you preserve inference compute credits and speed up the development process. Please use \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" for the iterative refinement and decomposition models, and \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\" for the fusion model.\n",
        "\n",
        "Deliverable: In the `cs329a_hw3/multi_lm_agent.py` file, implement the `run_pipeline` method, which takes a query and returns a final response.\n",
        "\n",
        "**Note**: How does the performance on the dataset compare between single-call LMs vs. the complete pipeline with the multi-LM agent? How does it improve accuracy by improving access to tool APIs and allowing for more complex reasoning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first evaluate over the entire dataset with just a zero-shot model\n",
        "- **IMPORTANT**: Make sure to evaluate over the entire dataset only when you are confident with your implementation! \n",
        "- This will help you preserve inference compute credits and speed up the development process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT5JPpsxlhSz"
      },
      "outputs": [],
      "source": [
        "# Let's first evaluate the zero-shot performance over the entire dataset\n",
        "# IMPORTANT: Make sure to evaluate over the entire dataset when you are confident with your implementation! \n",
        "# This will help you preserve inference compute credits and speed up the development process.\n",
        "\n",
        "from cs329a_hw3.evaluation import evaluate_qa, prepare_dataset\n",
        "\n",
        "debug_mode = False # Loads the entire dataset for evaluation.\n",
        "dataset = prepare_dataset(debug_mode=debug_mode)\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "\n",
        "zero_shot_responses, num_correct = [], 0\n",
        "for query, answer in zip(queries, answers):\n",
        "    print(f\"Query: {query}\")\n",
        "    response = multi_lm_agent.generate(query, model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\")\n",
        "    zero_shot_responses.append(response)\n",
        "    print(f\"Zero-Shot Response: {response}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    is_correct = evaluate_qa(query, response, answer, model=\"google/gemma-3n-E4B-it\")\n",
        "    num_correct += is_correct\n",
        "    print(\"Is Correct:\", is_correct)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Evaluating full zero-shot performance...\")\n",
        "complete_set_zero_shot_accuracy = num_correct / len(queries)\n",
        "print(f\"Accuracy = {complete_set_zero_shot_accuracy*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's evaluate over the entire dataset with our multi-LM agentic pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taP3V7VFlhSz"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.evaluation import evaluate_qa, prepare_dataset\n",
        "\n",
        "dataset = prepare_dataset(debug_mode=False)\n",
        "queries, answers = dataset['query'], dataset['answer']\n",
        "\n",
        "multi_lm_agent = MultiLMAgent(api_manager)\n",
        "\n",
        "multi_lm_responses, num_correct = [], 0\n",
        "print(\"\\nGenerating responses with Multi-LM Agent...\")\n",
        "for query, answer in zip(queries, answers):\n",
        "    print(f\"Query: {query}\")\n",
        "    response = multi_lm_agent.run_pipeline(query)\n",
        "    multi_lm_responses.append(response)\n",
        "    print(f\"Multi-LM Response: {response}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    is_correct = evaluate_qa(query, response, answer, model=\"google/gemma-3n-E4B-it\")\n",
        "    num_correct += is_correct\n",
        "    print(\"Is Correct:\", is_correct)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "complete_set_multi_lm_agent_accuracy = num_correct / len(queries)\n",
        "print(f\"Accuracy = {complete_set_multi_lm_agent_accuracy*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WkHNInVlhSz"
      },
      "source": [
        "## Part 4 - Deep Research Agent [20 points]\n",
        "\n",
        "Agentic LM systems are used everywhere today! From chatbots to coding agents to task automation, they are becoming more and more prevalent in our daily lives. Gemini, OpenAI, and Perplexity (among others) have \"deep research\" agents that are capable of synthesizing large amounts of online information and completing multi-step research tasks: [Introducing Deep Research](https://openai.com/index/introducing-deep-research/), [Deep Research](https://blog.google/products/gemini/google-gemini-deep-research/).\n",
        "\n",
        "Using the components we've built and extending them if needed, implement your own deep research agent that can generate comprehensive analyses from online sources. The agent should be able to handle complex queries requiring multi-step research, synthesizing information from multiple sources, and generating a comprehensive final report.\n",
        "\n",
        "**Key requirements for implementation:**\n",
        "- Generate a four-five paragraph report\n",
        "- Proper, easy-to-read structuring of the report\n",
        "- Usage of multiple sources of information with appropriate link citations\n",
        "- Track temporal information and maintain chronological accuracy\n",
        "\n",
        "Deliverable: In the `cs329a_hw3/deep_research_agent.py` file, implement the `research` method in the `DeepResearchAgent` class.\n",
        "\n",
        "The method should:\n",
        "- Take a complex query (e.g., \"What was the UK's macroeconomic performance in 2024?\")\n",
        "- Break it down into sub-questions\n",
        "- Research each sub-question using the search engine API\n",
        "- Synthesize and summarize findings with appropriate formatting as a report\n",
        "- Return a report and list of sources \n",
        "- **IMPORTANT**: Make sure to use cheaper models during the development process to help you preserve inference compute credits and speed up the process!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qT78-YulhSz"
      },
      "outputs": [],
      "source": [
        "from cs329a_hw3.DeepResearchAgent import DeepResearchAgent\n",
        "\n",
        "test_queries = [\n",
        "    \"What are the key developments and challenges in solid-state battery technology for electric vehicles in 2024, including major company announcements and technical breakthroughs?\",\n",
        "    \"How has the implementation of the UK's post-Brexit immigration policy affected its labor market and key industries between 2021-2024? Include specific policy changes and their measured impacts.\",\n",
        "    \"What progress has been made in nuclear fusion energy in 2024, focusing on major research milestones, private sector investments, and timeline predictions for commercial viability?\"\n",
        "]\n",
        "\n",
        "custom_agent = DeepResearchAgent(api_manager)\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    single_LM_response = custom_agent.generate(query, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")\n",
        "    report = custom_agent.research(query)\n",
        "    print(f\"Single-LM Response: {single_LM_response}\")\n",
        "    print(\"\\n\" * 3)\n",
        "    print(f\"Report: {report['report']}\")\n",
        "    print(f\"Sources: {report['sources']}\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jon-hw3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
